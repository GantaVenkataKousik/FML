{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMc3Qkh35nsUnooujy8RSOH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Venkatakousik/FML/blob/main/Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization using split method"
      ],
      "metadata": {
        "id": "7NcS_ciVljrq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8TNTUfsivtY",
        "outputId": "042b7061-c3c2-42d5-853e-b29bcfb83e65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization using split(): ['This', 'is', 'a', 'sample', 'sentence.', 'And', 'here', 'is', 'another', 'one!']\n",
            "Sentence Tokenization using split(): ['This is a sample sentence', 'And here is another one!']\n"
          ]
        }
      ],
      "source": [
        "text = \"This is a sample sentence. And here is another one!\"\n",
        "\n",
        "# Word tokenization using split()\n",
        "words = text.split()\n",
        "print(\"Word Tokenization using split():\", words)\n",
        "\n",
        "# Sentence tokenization using split()\n",
        "sentences = text.split(\".\")\n",
        "sentences = [s.strip() for s in sentences if s.strip()]\n",
        "print(\"Sentence Tokenization using split():\", sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization using Regular Expression\n"
      ],
      "metadata": {
        "id": "NCf0eWqTl7x3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word tokenization using the re library's findall() method with the regular expression pattern \\w+ captures consecutive sequences of alphanumeric characters (including underscores) as words. It handles basic cases well.\n",
        "\n",
        "\n",
        "Sentence tokenization using the re library's split() method with the regular expression pattern r'(?<=\\.)\\s' splits sentences based on a period followed by whitespace. It considers leading/trailing whitespace and avoids splitting abbreviations and decimal numbers"
      ],
      "metadata": {
        "id": "XHVt06ybmxCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"This is a sample sentence. And here is another one!\"\n",
        "\n",
        "# Word tokenization using re\n",
        "words = re.findall(r'\\w+', text)\n",
        "print(\"Word Tokenization using re:\", words)\n",
        "\n",
        "# Sentence tokenization using re\n",
        "sentences = re.split(r'(?<=\\.)', text)\n",
        "sentences = [s.strip() for s in sentences if s.strip()]\n",
        "print(\"Sentence Tokenization using re:\", sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbXhrc9KlipR",
        "outputId": "3976618d-a420-4bfe-b404-fa26890a666a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization using re: ['This', 'is', 'a', 'sample', 'sentence', 'And', 'here', 'is', 'another', 'one']\n",
            "Sentence Tokenization using re: ['This is a sample sentence.', 'And here is another one!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word tokenization using the nltk library's word_tokenize() method is robust. It tokenizes words effectively, even handling complex cases like contractions, punctuation marks, and special characters.\n",
        "\n",
        "\n",
        "Sentence tokenization using the nltk library's sent_tokenize() method performs well. It considers various punctuation marks and common sentence structures to accurately split sentences."
      ],
      "metadata": {
        "id": "pBZdAgsanVfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajruCMEgnikn",
        "outputId": "a736d5a6-596c-4bcd-8a9c-845b958fffa9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"This is a sample sentence.And here is another one!\"\n",
        "\n",
        "# Word tokenization using nltk\n",
        "words = word_tokenize(text)\n",
        "print(\"Word Tokenization using nltk:\", words)\n",
        "\n",
        "# Sentence tokenization using nltk\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization using nltk:\", sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3Elg8zFnS2R",
        "outputId": "ed354c25-532f-4e49-cd31-215008151227"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization using nltk: ['This', 'is', 'a', 'sample', 'sentence.And', 'here', 'is', 'another', 'one', '!']\n",
            "Sentence Tokenization using nltk: ['This is a sample sentence.And here is another one!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to tokenize the sentence based on periods while preserving abbreviations and decimal numbers, you can use the PunktSentenceTokenizer from the nltk library. The PunktSentenceTokenizer is a pre-trained model specifically designed for sentence tokenization."
      ],
      "metadata": {
        "id": "hzas37YNn5tK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4zcLOcuoniWC"
      }
    }
  ]
}