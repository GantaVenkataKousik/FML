{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEpz0fdrrdCEJ5sUzY1z3B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Venkatakousik/FML/blob/main/Stemming%26Lemmatization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEMMING\n"
      ],
      "metadata": {
        "id": "t4h-uZQ2eZTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install NLTK by running the following command:"
      ],
      "metadata": {
        "id": "rYpd0g67cUO6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SARUGDaKbv4A",
        "outputId": "e1b6fc15-1494-4341-e1ce-d1ee9577116f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "#importing the stemmer(PorterStemmer)\n",
        "#wordtokenize the token the words\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# punkt: The punkt resource is required for NLTK's tokenization module.\n",
        "#  Tokenization is the process of splitting text into individual words or tokens.\n",
        "# It provides a pre-trained model for tokenizing text in various languages.\n",
        "# By downloading punkt, you ensure that NLTK has access to the necessary resources for tokenization.\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# The averaged_perceptron_tagger provides a pre-trained model for POS tagging in English\n",
        "# POS tagging is the process of assigning grammatical tags to words in a sentence, such as noun, verb, adjective, etc.\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#  The wordnet resource is a large lexical database of English words, which includes information such as synonyms, antonyms, and word definitions.\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkSCsXqUcW5c",
        "outputId": "ed9d500e-25af-48da-fed7-a0c1fdcad151"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function that takes a sentence and returns the stemmed sentence using the PorterStemmer:\n",
        "def stem_sentence(sentence):\n",
        "    # creating an object on porterstemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    tokenized_words = word_tokenize(sentence)\n",
        "    stemmed_words = [stemmer.stem(word) for word in tokenized_words]\n",
        "    stemmed_sentence = ' '.join(stemmed_words)\n",
        "    return stemmed_sentence"
      ],
      "metadata": {
        "id": "7vPeaGpVcp1W"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I am running and eating a deliciously juicy apple.\"\n",
        "stemmed_sentence = stem_sentence(sentence)\n",
        "print(stemmed_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePPfISMxeID3",
        "outputId": "36d96db4-14f3-4908-eb19-23408be33470"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i am run and eat a delici juici appl .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LEMMATIZATION"
      ],
      "metadata": {
        "id": "QlWUsfN9ehVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The overall process\n",
        "# Even the code is also same\n",
        "# The only difference is we\n",
        "# create an object on WordNetLemmatizer and use the method in it\n",
        "# called lemmatize"
      ],
      "metadata": {
        "id": "-9PwtJdzeNoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "j1o8IyoofGmS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creatitn the fucntion to perform lemmatization\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokenized_words= word_tokenize(sentence)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokenized_words]\n",
        "    lemmatized_sentence  = ' '.join(lemmatized_words)\n",
        "    return lemmatized_sentence"
      ],
      "metadata": {
        "id": "yudNoDUgfMl1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I am running and eating a deliciously juicy apple.\"\n",
        "lemmatized_sentence = lemmatize_sentence(sentence)\n",
        "print(lemmatized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hGGBj9wf5oR",
        "outputId": "0ff43fdc-a82f-4c12-800f-a7ac79076aec"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am running and eating a deliciously juicy apple .\n"
          ]
        }
      ]
    }
  ]
}